Need to change two files inside the timm package (change {user-name} to your user name path e.g. home/tom.rahav/...):

file /home/{user-name}/miniconda3/envs/master/lib/python3.10/site-packages/timm/loss/cross_entropy.py
Change line 22 from (no need to unsqueeze in our format):
nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)
to:
nll_loss = -logprobs.gather(dim=-1, index=target)

file /home/{user-name}/miniconda3/envs/master/lib/python3.10/site-packages/timm/models/vision_transformer.py
Change line 45 from (change default input size):
'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,
to:
'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': None,

Current srun command:
Supervised: python train.py --model vit_small_patch16_224 --dataset TCGA --epochs 100 --target ER --workers 2 --num-classes 2 --log-wandb --experiment Supervised --subexperiment 1.0
SSL: python train.py --model vit_small_patch16_224_dino --dataset TCGA --epochs 100 --target ER --workers 2 --num-classes 2 --log-wandb --experiment SSl --subexperiment 1.0
Current sbatch command:
Supervised: ./py-sbatch.sh train.py --model vit_small_patch16_224 --dataset TCGA --epochs 100 --target ER --workers 2 --num-classes 2 --log-wandb --experiment Supervised --subexperiment 0.1
SSL: ./py-sbatch.sh train.py --model vit_small_patch16_224_dino --dataset CAT --epochs 500 --batch_size 1024 --target ER --num-classes 2 --log-wandb --experiment SSL --subexperiment 0.1

Supervised-Baseline:
./sbatch.sh train.py --model vit_small_patch16_224 --dataset CAT --epochs 500 --target ER --num-classes 2 --batch-size 256 --workers 2 --supervised --log-wandb --experiment Supervised --subexperiment 1.1
--lr-base 0.001 --opt adam --warmup-epochs 20

Supervised-fine-tune-no-grad:
./sbatch.sh train.py --model vit_small_patch16_224 --dataset CAT --epochs 500 --target ER --num-classes 2 --batch-size 256 --workers 2 --supervised --log-wandb --experiment Supervised --subexperiment 1.2
 --initial-checkpoint "/home/noam.moshe/model_best.pth.tar" --no-grad --lr-base 0.001 --sched cosine --warmup-epochs 20

Supervised-fine-tune-grad:
./sbatch.sh train.py --model vit_small_patch16_224 --dataset CAT --epochs 500 --target ER --num-classes 2 --batch-size 256 --workers 2 --supervised --log-wandb --experiment Supervised --subexperiment 1.3
 --initial-checkpoint "/home/noam.moshe/model_best.pth.tar" --lr-base 0.001 --sched cosine --warmup-epochs 20

(old:)
./py-sbatch.sh train.py --model vit_small_patch16_224_dino --dataset TCGA --epochs 30 --batch-size 10 --target ER --workers 2 --num-classes 2 --balanced_dataset --log-wandb --experiment Supervised --subexperiment 1.0
